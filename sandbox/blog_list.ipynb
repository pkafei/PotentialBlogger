{
 "metadata": {
  "name": "",
  "signature": "sha256:247df189d36d7be264456248d7ae445a256aee62e68bb5bd156c9d948ade8f92"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "from bs\n",
      "from urllib2 import *\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Compare the Print Out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://dailytekk.com/2013/11/18/the-100-best-most-interesting-blogs-and-websites-of-2014-2/\"\n",
      "\n",
      "def get_category_links(section_url):\n",
      "    header = {'User-Agent': 'Mozilla/5.0'}\n",
      "    req = urllib2.Request(BASE_URL, headers=header)\n",
      "    page = urllib2.urlopen(req)\n",
      "    soup = BeautifulSoup(page, \"lxml\")\n",
      "    title = soup.find(\"div\", \"entry-content\")\n",
      "    category_titles = title.findAll(\"h2\")\n",
      "    #category_links = [BASE_URL + p.a[\"href\"] for p in boccat.findAll(\"p\")]\n",
      "    ct = [tag.name for tag in category_titles]\n",
      "    print ct\n",
      "    print category_titles\n",
      "    return category_titles\n",
      "\n",
      "get_category_links(BASE_URL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['h2', 'h2', 'h2', 'h2', 'h2', 'h2', 'h2', 'h2', 'h2', 'h2', 'h2']\n",
        "[<h2>Past Highlights\u2026</h2>, <h2>Must Reads</h2>, <h2>For When You\u2019re Bored</h2>, <h2>Design, Art, Photography</h2>, <h2>Lifestyle</h2>, <h2>That\u2019s Just Cool\u2026</h2>, <h2>Learn Something</h2>, <h2>Current Events</h2>, <h2>Culture &amp; Trends</h2>, <h2>Technology</h2>, <h2>Absolutely Don\u2019t Miss</h2>]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[<h2>Past Highlights\u2026</h2>,\n",
        " <h2>Must Reads</h2>,\n",
        " <h2>For When You\u2019re Bored</h2>,\n",
        " <h2>Design, Art, Photography</h2>,\n",
        " <h2>Lifestyle</h2>,\n",
        " <h2>That\u2019s Just Cool\u2026</h2>,\n",
        " <h2>Learn Something</h2>,\n",
        " <h2>Current Events</h2>,\n",
        " <h2>Culture &amp; Trends</h2>,\n",
        " <h2>Technology</h2>,\n",
        " <h2>Absolutely Don\u2019t Miss</h2>]"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experimental Code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://dailytekk.com/2013/11/18/the-100-best-most-interesting-blogs-and-websites-of-2014-2/\"\n",
      "\n",
      "'''def get_category_links(section_url):\n",
      "    header = {'User-Agent': 'Mozilla/5.0'}\n",
      "    req = urllib2.Request(BASE_URL, headers=header)\n",
      "    page = urllib2.urlopen(req)\n",
      "    soup = BeautifulSoup(page, \"lxml\")\n",
      "    title = soup.find(\"div\", \"entry-content\")\n",
      "    category_titles = title.findAll(\"h2\")\n",
      "    #category_links = [BASE_URL + p.a[\"href\"] for p in boccat.findAll(\"p\")]\n",
      "    #print category_titles\n",
      "    return category_titles '''\n",
      "\n",
      "def clean_category_links(category_titles):\n",
      "    header = {'User-Agent': 'Mozilla/5.0'}\n",
      "    req = urllib2.Request(BASE_URL, headers=header)\n",
      "    page = urllib2.urlopen(req)\n",
      "    soup = BeautifulSoup(page, \"lxml\")\n",
      "    title = soup.find(\"div\", \"entry-content\")\n",
      "    category_titles = title.findAll(\"h2\")\n",
      "    ct = [category_titles.replace('h2', '') for c in category_titles]\n",
      "    print ct\n",
      "    return ct\n",
      "\n",
      "clean_category_links(BASE_URL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'ResultSet' object has no attribute 'replace'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-28-9785fa9b35a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mclean_category_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-28-9785fa9b35a2>\u001b[0m in \u001b[0;36mclean_category_links\u001b[0;34m(category_titles)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entry-content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcategory_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcategory_titles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_titles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'ResultSet' object has no attribute 'replace'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "html = \"<p>Good, <b>bad</b>, and <i>ug<b>l</b><u>y</u></i></p>\"\n",
      "invalid_tags = ['b', 'i', 'u', 'p', 'body', 'html']\n",
      "soup = BeautifulSoup(html)\n",
      "for tag in invalid_tags: \n",
      "    for match in soup.findAll(tag):\n",
      "        match.replaceWithChildren()\n",
      "print soup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## The more Advanced Example "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://dailytekk.com/2013/11/18/the-100-best-most-interesting-blogs-and-websites-of-2014-2/\"\n",
      "\n",
      "def get_category_links(section_url):\n",
      "    header = {'User-Agent': 'Mozilla/5.0'}\n",
      "    req = urllib2.Request(BASE_URL, headers=header)\n",
      "    page = urllib2.urlopen(req)\n",
      "    soup = BeautifulSoup(page, \"lxml\")\n",
      "    boccat = soup.find(\"div\", \"entry-content content\")\n",
      "    category_links = [BASE_URL + p.a[\"href\"] for p in boccat.findAll(\"p\")]\n",
      "    print category_links\n",
      "    return category_links\n",
      "\n",
      "#winner = [h2.string for h2 in soup.findAll(\"h2\", \"boc1\")]\n",
      "\n",
      "get_category_links(BASE_URL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'NoneType' object has no attribute '__getitem__'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-1c952be74d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mget_category_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-4-1c952be74d64>\u001b[0m in \u001b[0;36mget_category_links\u001b[0;34m(section_url)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mboccat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entry-content content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcategory_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBASE_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboccat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mcategory_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcategory_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Scraping from Other Examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example from Greg's blog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://www.chicagoreader.com\"\n",
      "\n",
      "def get_category_links(section_url):\n",
      "    html = urlopen(section_url).read()\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "    boccat = soup.find(\"dl\", \"boccat\")\n",
      "    category_links = [BASE_URL + p.a[\"href\"] for p in boccat.findAll(\"dd\")]\n",
      "    print category_links\n",
      "    return category_links\n",
      "\n",
      "\n",
      "get_category_links(BASE_URL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'NoneType' object has no attribute 'findAll'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-ecbd03a0745e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mget_category_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-5-ecbd03a0745e>\u001b[0m in \u001b[0;36mget_category_links\u001b[0;34m(section_url)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mboccat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"boccat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcategory_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBASE_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboccat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mcategory_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcategory_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stack Overflow List Comprehension"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "html = \"<p>Good, <b>bad</b>, and <i>ug<b>l</b><u>y</u></i></p>\"\n",
      "invalid_tags = ['b', 'i', 'u', 'p', 'body', 'html']\n",
      "soup = BeautifulSoup(html)\n",
      "for tag in invalid_tags: \n",
      "    for match in soup.findAll(tag):\n",
      "        match.replaceWithChildren()\n",
      "print soup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Good, bad, and ugly\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Sources:\n",
      "    \n",
      "https://gist.github.com/gjreda/f3e6875f869779ec03db/download#\n",
      "http://dailytekk.com/2013/11/18/the-100-best-most-interesting-blogs-and-websites-of-2014-2/\n",
      "http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}